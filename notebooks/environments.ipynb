{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_sOmCoOrF0F8"
      },
      "outputs": [],
      "source": [
        "#@title Import Brax and some helper modules\n",
        "\n",
        "import functools\n",
        "import time\n",
        "\n",
        "from IPython.display import HTML, Image \n",
        "import gym\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  from IPython.display import clear_output \n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax import jumpy as jp\n",
        "from brax.envs import to_torch\n",
        "from brax.io import html\n",
        "from brax.io import image\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "import torch\n",
        "v = torch.ones(1, device='cuda')  # init torch cuda before jax\n",
        "\n",
        "import sys\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import argparse, math, os\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "import functools\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "\n",
        "pi = Variable(torch.FloatTensor([math.pi])).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "\n",
        "class NormalizedActions(gym.ActionWrapper):\n",
        "\n",
        "    def action(self, action):\n",
        "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
        "        action *= (self.action_space.high - self.action_space.low)\n",
        "        action += self.action_space.low\n",
        "        return action\n",
        "\n",
        "    def reverse_action(self, action):\n",
        "        action -= self.action_space.low\n",
        "        action /= (self.action_space.high - self.action_space.low)\n",
        "        action = action * 2 - 1\n",
        "        return actions\n"
      ],
      "metadata": {
        "id": "EvMfw-aS8OzG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normal(x, mu, sigma_sq):\n",
        "    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()\n",
        "    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()\n",
        "    return a*b\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, hidden_size, num_inputs, action_space):\n",
        "        super(Policy, self).__init__()\n",
        "        self.action_space = action_space\n",
        "        num_outputs = action_space.shape[0]\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
        "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = inputs\n",
        "        x = F.relu(self.linear1(x))\n",
        "        mu = self.linear2(x)\n",
        "        sigma_sq = self.linear2_(x)\n",
        "\n",
        "        return mu, sigma_sq\n",
        "\n",
        "\n",
        "class REINFORCE:\n",
        "    def __init__(self, hidden_size, num_inputs, action_space):\n",
        "        self.action_space = action_space\n",
        "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
        "        self.model = self.model.cuda()\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
        "        self.model.train()\n",
        "\n",
        "    def select_action(self, state):\n",
        "        mu, sigma_sq = self.model(Variable(state).cuda())\n",
        "        sigma_sq = F.softplus(sigma_sq)\n",
        "\n",
        "        eps = torch.randn(mu.size())\n",
        "        # calculate the probability\n",
        "        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
        "        prob = normal(action, mu, sigma_sq)\n",
        "        entropy = -0.5*((sigma_sq+2*pi.expand_as(sigma_sq)).log()+1)\n",
        "\n",
        "        log_prob = prob.log()\n",
        "        return action, log_prob, entropy\n",
        "\n",
        "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
        "        R = torch.zeros(1, 1)\n",
        "        loss = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            R = gamma * R + rewards[i]\n",
        "            loss = loss - (log_probs[i]*(Variable(R).expand_as(log_probs[i])).cuda()).sum() - (0.0001*entropies[i].cuda()).sum()\n",
        "        loss = loss / len(rewards)\n",
        "\t\t\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
        "        self.optimizer.step()\n"
      ],
      "metadata": {
        "id": "F8MxQcFu8R0A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seed = 0\n",
        "gamma = 0.99\n",
        "exploration_end = 100\n",
        "num_steps = 1000\n",
        "num_episodes = 2000\n",
        "hidden_size = 128\n",
        "\n",
        "\n",
        "entry_point = functools.partial(envs.create_gym_env, env_name='reacher')\n",
        "if 'brax-reacher-v0' not in gym.envs.registry.env_specs:\n",
        "    gym.register('brax-reacher-v0', entry_point=entry_point)\n",
        "env = gym.make('brax-reacher-v0')\n",
        "env = to_torch.JaxToTorchWrapper(env, device='cpu')\n",
        "\n",
        "# if type(env.action_space) != gym.spaces.discrete.Discrete:\n",
        "#     from reinforce_continuous import REINFORCE\n",
        "#     # env = NormalizedActions(gym.make(env_name))\n",
        "# else:\n",
        "#     from reinforce_discrete import REINFORCE\n",
        "\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "agent = REINFORCE(hidden_size, env.observation_space.shape[0], env.action_space)\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    state = env.reset()[None]\n",
        "    entropies = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    for t in range(num_steps):\n",
        "        action, log_prob, entropy = agent.select_action(state)\n",
        "        action = action.cpu()\n",
        "        \n",
        "        next_state, reward, done, _ = env.step(action.numpy()[0])\n",
        "        done = done.cpu().numpy().item()\n",
        "        entropies.append(entropy)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward.cpu().numpy().item())\n",
        "        state = next_state[None]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    agent.update_parameters(rewards, log_probs, entropies, gamma)\n",
        "\n",
        "    print(\"Episode: {}, reward: {}\".format(i_episode, np.sum(rewards)))\n",
        "\t\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8r_Tyu_A8MFq",
        "outputId": "00e08a56-1431-4dd2-f45e-be0531dbfe1c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, reward: -1571.221885085106\n",
            "Episode: 1, reward: -1640.5578821897507\n",
            "Episode: 2, reward: -1647.704906783998\n",
            "Episode: 3, reward: -1545.376258423552\n",
            "Episode: 4, reward: -1647.3130498751998\n",
            "Episode: 5, reward: -1640.9833238646388\n",
            "Episode: 6, reward: -1698.7089726254344\n",
            "Episode: 7, reward: -1603.1306994650513\n",
            "Episode: 8, reward: -1524.0778157226741\n",
            "Episode: 9, reward: -1604.8992355391383\n",
            "Episode: 10, reward: -1569.7512127421796\n",
            "Episode: 11, reward: -1633.798154644668\n",
            "Episode: 12, reward: -1558.9657598733902\n",
            "Episode: 13, reward: -1547.2181352041662\n",
            "Episode: 14, reward: -1600.4012333378196\n",
            "Episode: 15, reward: -1559.8717135414481\n",
            "Episode: 16, reward: -1549.1581638380885\n",
            "Episode: 17, reward: -1637.0808537714183\n",
            "Episode: 18, reward: -1556.6965787038207\n",
            "Episode: 19, reward: -1560.3428805153817\n",
            "Episode: 20, reward: -1511.4160367697477\n",
            "Episode: 21, reward: -1591.6446281820536\n",
            "Episode: 22, reward: -1639.6424173228443\n",
            "Episode: 23, reward: -1576.5129375085235\n",
            "Episode: 24, reward: -1658.9999872483313\n",
            "Episode: 25, reward: -1586.992974333465\n",
            "Episode: 26, reward: -1599.143102703616\n",
            "Episode: 27, reward: -1631.9295566305518\n",
            "Episode: 28, reward: -1759.3153522014618\n",
            "Episode: 29, reward: -1671.4945318549871\n",
            "Episode: 30, reward: -1616.7289371229708\n",
            "Episode: 31, reward: -1600.4528841115534\n",
            "Episode: 32, reward: -1782.7876791507006\n",
            "Episode: 33, reward: -1595.5235533192754\n",
            "Episode: 34, reward: -1679.2398717030883\n",
            "Episode: 35, reward: -1630.6149736866355\n",
            "Episode: 36, reward: -1732.3587437570095\n",
            "Episode: 37, reward: -1604.6698709279299\n",
            "Episode: 38, reward: -1554.7798174768686\n",
            "Episode: 39, reward: -1670.0893589481711\n",
            "Episode: 40, reward: -1600.7627031132579\n",
            "Episode: 41, reward: -1687.5489490022883\n",
            "Episode: 42, reward: -1602.288182541728\n",
            "Episode: 43, reward: -1571.2199499122798\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-020ca501e5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode: {}, reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-e58c25f766ff>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, rewards, log_probs, entropies, gamma)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Brax Environments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}