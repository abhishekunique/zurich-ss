{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_sOmCoOrF0F8"
   },
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "\n",
    "import functools\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, Image \n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  from IPython.display import clear_output \n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax import jumpy as jp\n",
    "from brax.envs import to_torch\n",
    "from brax.io import html\n",
    "from brax.io import image\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F8MxQcFu8R0A"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[1]\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.cuda()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(Variable(state).cuda())\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "        eps = torch.randn(mu.size())\n",
    "        action = (mu + sigma_sq.sqrt()*Variable(eps).cuda()).data\n",
    "        prob = torch.distributions.normal.Normal(mu, sigma_sq.sqrt())\n",
    "        entropy = prob.entropy().sum(axis=-1)\n",
    "        log_prob = prob.log_prob(action).sum(axis=-1)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        # Bookkeeping\n",
    "        R = torch.zeros(rewards.shape[0],rewards.shape[1]).cuda()\n",
    "        running_r = torch.zeros(rewards.shape[0],).cuda()\n",
    "        loss = 0\n",
    "        R_EPS = 1e-9\n",
    "        \n",
    "        # Compute discounted cumulative sum TODO: Check this\n",
    "        for i in reversed(range(rewards.shape[1])):\n",
    "            running_r = gamma * running_r + rewards[:, i]\n",
    "            R[:, i] = running_r\n",
    "            \n",
    "        # Normalize advantages\n",
    "        R_mean = torch.mean(R)\n",
    "        R_std = torch.std(R)\n",
    "        R = (R - R_mean) / (R_std + R_EPS)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -(log_probs*R).sum() - 0.0001*entropies.sum()\n",
    "        loss = loss.sum(axis=-1) / len(rewards)\n",
    "\n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "#         utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8r_Tyu_A8MFq",
    "outputId": "00e08a56-1431-4dd2-f45e-be0531dbfe1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: -1604.8577880859375\n",
      "Episode: 1, reward: -1564.8717041015625\n",
      "Episode: 2, reward: -1555.08447265625\n",
      "Episode: 3, reward: -1525.9417724609375\n",
      "Episode: 4, reward: -1509.2880859375\n",
      "Episode: 5, reward: -1475.717529296875\n",
      "Episode: 6, reward: -1461.998291015625\n",
      "Episode: 7, reward: -1444.377197265625\n",
      "Episode: 8, reward: -1427.8492431640625\n",
      "Episode: 9, reward: -1410.3984375\n",
      "Episode: 10, reward: -1381.02197265625\n",
      "Episode: 11, reward: -1356.1070556640625\n",
      "Episode: 12, reward: -1337.4871826171875\n",
      "Episode: 13, reward: -1318.907470703125\n",
      "Episode: 14, reward: -1295.4886474609375\n",
      "Episode: 15, reward: -1269.55322265625\n",
      "Episode: 16, reward: -1242.878173828125\n",
      "Episode: 17, reward: -1211.090576171875\n",
      "Episode: 18, reward: -1183.08447265625\n",
      "Episode: 19, reward: -1148.06494140625\n",
      "Episode: 20, reward: -1122.9906005859375\n",
      "Episode: 21, reward: -1082.00732421875\n",
      "Episode: 22, reward: -1054.793212890625\n",
      "Episode: 23, reward: -1025.46630859375\n",
      "Episode: 24, reward: -1004.3538818359375\n",
      "Episode: 25, reward: -980.76220703125\n",
      "Episode: 26, reward: -963.7935791015625\n",
      "Episode: 27, reward: -934.913818359375\n",
      "Episode: 28, reward: -913.37890625\n",
      "Episode: 29, reward: -896.2777099609375\n",
      "Episode: 30, reward: -883.0048217773438\n",
      "Episode: 31, reward: -873.6507568359375\n",
      "Episode: 32, reward: -851.2938842773438\n",
      "Episode: 33, reward: -834.5962524414062\n",
      "Episode: 34, reward: -818.961181640625\n",
      "Episode: 35, reward: -794.209716796875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30412/618841045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30412/714280725.py\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30412/714280725.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0msigma_sq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_full_backward_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1164\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "gamma = 0.99\n",
    "exploration_end = 100\n",
    "num_steps = 1000\n",
    "num_episodes = 2000\n",
    "hidden_size = 128\n",
    "num_envs = 100\n",
    "\n",
    "entry_point = functools.partial(envs.create_gym_env, env_name='reacher')\n",
    "if 'brax-reacher-v0' not in gym.envs.registry.env_specs:\n",
    "    gym.register('brax-reacher-v0', entry_point=entry_point)\n",
    "env = gym.make('brax-reacher-v0', batch_size=num_envs, episode_length=num_steps)\n",
    "env = to_torch.JaxToTorchWrapper(env, device='cuda')\n",
    "\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "agent = REINFORCE(hidden_size, env.observation_space.shape[1], env.action_space)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    entropies = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for t in range(num_steps):\n",
    "        action, log_prob, entropy = agent.select_action(state)\n",
    "        action = action.cpu()\n",
    "    \n",
    "        next_state, reward, done, _ = env.step(action.numpy())\n",
    "        entropies.append(entropy)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "\n",
    "    # Bookkeeping\n",
    "    states = torch.cat([s[None] for s in states])\n",
    "    states = torch.transpose(states, 1, 0)\n",
    "    \n",
    "    actions = torch.cat([a[None] for a in actions])\n",
    "    actions = torch.transpose(actions, 1, 0)\n",
    "    \n",
    "    rewards = torch.cat([r[None] for r in rewards])\n",
    "    rewards = torch.transpose(rewards, 1, 0)\n",
    "    \n",
    "    entropies = torch.cat([e[None] for e in entropies])\n",
    "    entropies = torch.transpose(entropies, 1, 0)\n",
    "    \n",
    "    log_probs = torch.cat([lp[None] for lp in log_probs])\n",
    "    log_probs = torch.transpose(log_probs, 1, 0)\n",
    "    \n",
    "    agent.update_parameters(rewards, log_probs, entropies, gamma)\n",
    "    \n",
    "    rewards_np = rewards.cpu().numpy().sum(axis=-1).mean()\n",
    "    print(\"Episode: {}, reward: {}\".format(i_episode, rewards_np))\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Brax Environments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
