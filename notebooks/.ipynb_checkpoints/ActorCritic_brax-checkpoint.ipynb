{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_sOmCoOrF0F8"
   },
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "\n",
    "import functools\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, Image \n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  from IPython.display import clear_output \n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax import jumpy as jp\n",
    "from brax.envs import to_torch\n",
    "from brax.io import html\n",
    "from brax.io import image\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "F8MxQcFu8R0A"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store environment transitions.\"\"\"\n",
    "    def __init__(self, obs_size, action_size, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        \n",
    "        self.obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
    "        self.next_obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
    "        self.actions = np.empty((capacity, action_size), dtype=np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
    "        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.last_save = 0\n",
    "        self.full = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.idx\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        np.copyto(self.obses[self.idx], obs)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_obses[self.idx], next_obs)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.full = self.full or self.idx == 0\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0,\n",
    "                                 self.capacity if self.full else self.idx,\n",
    "                                 size=batch_size)\n",
    "\n",
    "        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n",
    "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
    "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n",
    "        next_obses = torch.as_tensor(self.next_obses[idxs],\n",
    "                                     device=self.device).float()\n",
    "        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n",
    "\n",
    "        return obses, actions, rewards, next_obses, not_dones\n",
    "        \n",
    "class Qf(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs):\n",
    "        super(Qf, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, obs, action):\n",
    "        x = torch.cat([obs, action], dim=-1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        baseline_out = self.linear2(x)\n",
    "        return baseline_out\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[1]\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space, discount=0.99):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        self.discount = discount\n",
    "        \n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.cuda()\n",
    "        \n",
    "        self.qf1 = Qf(hidden_size, num_inputs + action_space.shape[1])\n",
    "        self.qf1 = self.qf1.cuda()\n",
    "        \n",
    "        self.target_qf1 = Qf(hidden_size, num_inputs + action_space.shape[1])\n",
    "        self.target_qf1 = self.target_qf1.cuda()\n",
    "        \n",
    "        self.model.train()\n",
    "        self.qf1.train()\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_sample_no_postprocess(self, loc, scale):\n",
    "        return torch.normal(loc, scale)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_entropy(self, loc, scale):\n",
    "        log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "        entropy = 0.5 + log_normalized\n",
    "        entropy = entropy * torch.ones_like(loc)\n",
    "        dist = torch.normal(loc, scale)\n",
    "        log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "        entropy = entropy + log_det_jacobian\n",
    "        return entropy.sum(dim=-1)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_log_prob(self, loc, scale, dist):\n",
    "        log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
    "        log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "        log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "        log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
    "        return log_prob.sum(dim=-1)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(state.cuda())\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "        sigma = sigma_sq.sqrt()\n",
    "        action = torch.tanh(self.dist_sample_no_postprocess(mu, sigma))\n",
    "        entropy = self.dist_entropy(mu,  sigma)\n",
    "        log_prob = self.dist_log_prob(mu,  sigma, action)\n",
    "        return action, log_prob, entropy\n",
    "        \n",
    "    # TODO: Check maximum entropy\n",
    "    @torch.jit.export\n",
    "    def compute_losses(self, obs_t, actions_t, rewards_t, next_obs_t, not_dones_t):\n",
    "        # Policy loss\n",
    "        mu, sigma_sq = self.model(obs_t)\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "        sigma = sigma_sq.sqrt()\n",
    "        new_obs_actions = torch.tanh(self.dist_sample_no_postprocess(mu, sigma))\n",
    "        entropy = self.dist_entropy(mu,  sigma)\n",
    "        log_pi = self.dist_log_prob(mu,  sigma, new_obs_actions)\n",
    "        \n",
    "        # TODO: Change this over to REINFORCE loss maybe instead of reparameterization??\n",
    "        q_new_actions = self.qf1(obs_t, new_obs_actions)\n",
    "        policy_loss = -q_new_actions.mean()\n",
    "\n",
    "        # Compute Bellman loss\n",
    "        q1_pred = self.qf1(obs_t, actions_t)\n",
    "        mu, sigma_sq = self.model(next_obs_t.cuda())\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "        sigma = sigma_sq.sqrt()\n",
    "        new_next_actions = torch.tanh(self.dist_sample_no_postprocess(mu, sigma))\n",
    "        new_log_pi = self.dist_log_prob(mu,  sigma, new_obs_actions)\n",
    "        \n",
    "        target_q_values = self.target_qf1(next_obs_t, new_next_actions)\n",
    "        q_target = rewards_t + not_dones_t * self.discount * target_q_values\n",
    "        \n",
    "        # L2 error on bellman\n",
    "        qf_loss = torch.linalg.norm(q1_pred - q_target.detach(), dim=-1).mean()\n",
    "\n",
    "        return policy_loss, qf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_target(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8r_Tyu_A8MFq",
    "outputId": "00e08a56-1431-4dd2-f45e-be0531dbfe1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  0\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 0, reward: -838.8779907226562\n",
      "Episode  1\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 1, reward: -833.17236328125\n",
      "Episode  2\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 2, reward: -840.987548828125\n",
      "Episode  3\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 3, reward: -831.6994018554688\n",
      "Episode  4\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 4, reward: -837.9450073242188\n",
      "Episode  5\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 5, reward: -834.2408447265625\n",
      "Episode  6\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n",
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 6, reward: -831.3707885742188\n",
      "Episode  7\n",
      "Update step number  0\n",
      "Update step number  1\n",
      "Update step number  2\n",
      "Update step number  3\n",
      "Update step number  4\n",
      "Update step number  5\n",
      "Update step number  6\n",
      "Update step number  7\n",
      "Update step number  8\n",
      "Update step number  9\n",
      "Update step number  10\n",
      "Update step number  11\n",
      "Update step number  12\n",
      "Update step number  13\n",
      "Update step number  14\n",
      "Update step number  15\n",
      "Update step number  16\n",
      "Update step number  17\n",
      "Update step number  18\n",
      "Update step number  19\n",
      "Update step number  20\n",
      "Update step number  21\n",
      "Update step number  22\n",
      "Update step number  23\n",
      "Update step number  24\n",
      "Update step number  25\n",
      "Update step number  26\n",
      "Update step number  27\n",
      "Update step number  28\n",
      "Update step number  29\n",
      "Update step number  30\n",
      "Update step number  31\n",
      "Update step number  32\n",
      "Update step number  33\n",
      "Update step number  34\n",
      "Update step number  35\n",
      "Update step number  36\n",
      "Update step number  37\n",
      "Update step number  38\n",
      "Update step number  39\n",
      "Update step number  40\n",
      "Update step number  41\n",
      "Update step number  42\n",
      "Update step number  43\n",
      "Update step number  44\n",
      "Update step number  45\n",
      "Update step number  46\n",
      "Update step number  47\n",
      "Update step number  48\n",
      "Update step number  49\n",
      "Update step number  50\n",
      "Update step number  51\n",
      "Update step number  52\n",
      "Update step number  53\n",
      "Update step number  54\n",
      "Update step number  55\n",
      "Update step number  56\n",
      "Update step number  57\n",
      "Update step number  58\n",
      "Update step number  59\n",
      "Update step number  60\n",
      "Update step number  61\n",
      "Update step number  62\n",
      "Update step number  63\n",
      "Update step number  64\n",
      "Update step number  65\n",
      "Update step number  66\n",
      "Update step number  67\n",
      "Update step number  68\n",
      "Update step number  69\n",
      "Update step number  70\n",
      "Update step number  71\n",
      "Update step number  72\n",
      "Update step number  73\n",
      "Update step number  74\n",
      "Update step number  75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update step number  76\n",
      "Update step number  77\n",
      "Update step number  78\n",
      "Update step number  79\n",
      "Update step number  80\n",
      "Update step number  81\n",
      "Update step number  82\n",
      "Update step number  83\n",
      "Update step number  84\n",
      "Update step number  85\n",
      "Update step number  86\n",
      "Update step number  87\n",
      "Update step number  88\n",
      "Update step number  89\n",
      "Update step number  90\n",
      "Update step number  91\n",
      "Update step number  92\n",
      "Update step number  93\n",
      "Update step number  94\n",
      "Update step number  95\n",
      "Update step number  96\n",
      "Update step number  97\n",
      "Update step number  98\n",
      "Update step number  99\n",
      "Episode: 7, reward: -838.4404907226562\n",
      "Episode  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24086/3736128221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# TODO: Check this for speed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/brax/envs/to_torch.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mObsType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/brax/envs/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/flax/struct.py\u001b[0m in \u001b[0;36mclz_from_iterable\u001b[0;34m(meta, data)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mclz_from_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mmeta_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mdata_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "gamma = torch.Tensor([0.99]).cuda()\n",
    "exploration_end = 100\n",
    "num_steps = 1000\n",
    "num_episodes = 2000\n",
    "hidden_size = 128\n",
    "num_envs = 100\n",
    "num_update_steps = 100\n",
    "capacity = 10000\n",
    "device = 'cuda'\n",
    "batch_size = 32\n",
    "target_flip_freq = 10\n",
    "target_flip_tau = 5e-3\n",
    "\n",
    "entry_point = functools.partial(envs.create_gym_env, env_name='reacher')\n",
    "if 'brax-reacher-v0' not in gym.envs.registry.env_specs:\n",
    "    gym.register('brax-reacher-v0', entry_point=entry_point)\n",
    "env = gym.make('brax-reacher-v0', batch_size=num_envs, episode_length=num_steps)\n",
    "env = to_torch.JaxToTorchWrapper(env, device='cuda')\n",
    "\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "replay_buffer = ReplayBuffer(env.observation_space.shape[1], \n",
    "                             env.action_space.shape[1], \n",
    "                             capacity, \n",
    "                             device)\n",
    "\n",
    "agent = ActorCritic(hidden_size, env.observation_space.shape[1], env.action_space)\n",
    "agent = torch.jit.script(agent)\n",
    "optimizer = optim.Adam(list(agent.model.parameters()) + list(agent.qf1.parameters()))\n",
    "    \n",
    "# Copy parameters initially\n",
    "flip_target(agent.qf1, agent.target_qf1, 1.0)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    print(\"Episode \", i_episode)\n",
    "    state = env.reset()\n",
    "    entropies = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for t in range(num_steps):\n",
    "        action, log_prob, entropy = agent.select_action(state)\n",
    "        action = action.cpu()\n",
    "    \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # TODO: Check this for speed. \n",
    "        for j in range(next_state.shape[0]):\n",
    "            replay_buffer.add(state.cpu().detach().numpy()[j], \n",
    "                              action.cpu().detach().numpy()[j], \n",
    "                              reward.cpu().detach().numpy()[j], \n",
    "                              next_state.cpu().detach().numpy()[j], \n",
    "                              done.cpu().detach().numpy()[j])\n",
    "        \n",
    "        entropies.append(entropy)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "\n",
    "    # Bookkeeping and logging\n",
    "    rewards = torch.cat([r[None] for r in rewards])\n",
    "    rewards = torch.transpose(rewards, 1, 0)\n",
    "    rewards_np = rewards.cpu().numpy().sum(axis=-1).mean()\n",
    "    print(\"Episode: {}, reward: {}\".format(i_episode, rewards_np))\n",
    "    \n",
    "    # Perform actor-critic update\n",
    "    for update_num in range(num_update_steps):\n",
    "        obs_t, actions_t, rewards_t, next_obs_t, not_dones_t = replay_buffer.sample(batch_size)\n",
    "        policy_loss, qf_loss = agent.compute_losses(obs_t, actions_t, rewards_t, next_obs_t, not_dones_t)\n",
    "        loss = policy_loss + qf_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Flip target network originally\n",
    "    if i_episode % target_flip_freq == 0:\n",
    "        flip_target(agent.qf1, agent.target_qf1, target_flip_tau)\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def sample_t(self, batch_size): \n",
    "        obses, actions, rewards, next_obses, not_dones = self.sample(batch_size)\n",
    "        import IPython\n",
    "        IPython.embed()\n",
    "        obs_t = torch.Tensor(obses).cuda()\n",
    "        actions_t = torch.Tensor(actions).cuda()\n",
    "        rewards_t = torch.Tensor(rewards).cuda()\n",
    "        next_obs_t = torch.Tensor(next_obses).cuda()\n",
    "        not_dones_t = torch.Tensor(not_dones).cuda()\n",
    "        return obs_t, actions_t, rewards_t, next_obs_t, not_dones_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Brax Environments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
