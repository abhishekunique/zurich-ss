{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_sOmCoOrF0F8"
   },
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "\n",
    "import functools\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, Image \n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  from IPython.display import clear_output \n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax import jumpy as jp\n",
    "from brax.envs import to_torch\n",
    "from brax.io import html\n",
    "from brax.io import image\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "pi = Variable(torch.FloatTensor([math.pi])).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "F8MxQcFu8R0A"
   },
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        baseline_out = self.linear2(x)\n",
    "        return baseline_out\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.shape[1]\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "        self.linear2_ = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu = self.linear2(x)\n",
    "        sigma_sq = self.linear2_(x)\n",
    "\n",
    "        return mu, sigma_sq\n",
    "\n",
    "\n",
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.cuda()\n",
    "        \n",
    "        self.baseline = Baseline(hidden_size, num_inputs)\n",
    "        self.baseline = self.baseline.cuda()\n",
    "\n",
    "        self.model.train()\n",
    "        self.baseline.train()\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_sample_no_postprocess(self, loc, scale):\n",
    "        return torch.normal(loc, scale)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_entropy(self, loc, scale):\n",
    "        log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "        entropy = 0.5 + log_normalized\n",
    "        entropy = entropy * torch.ones_like(loc)\n",
    "        dist = torch.normal(loc, scale)\n",
    "        log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "        entropy = entropy + log_det_jacobian\n",
    "        return entropy.sum(dim=-1)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def dist_log_prob(self, loc, scale, dist):\n",
    "        log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
    "        log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "        log_det_jacobian = 2 * (math.log(2) - dist - F.softplus(-2 * dist))\n",
    "        log_prob = log_unnormalized - log_normalized - log_det_jacobian\n",
    "        return log_prob.sum(dim=-1)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def select_action(self, state):\n",
    "        mu, sigma_sq = self.model(state.cuda())\n",
    "        sigma_sq = F.softplus(sigma_sq)\n",
    "        sigma = sigma_sq.sqrt()\n",
    "        action = torch.tanh(self.dist_sample_no_postprocess(mu, sigma))\n",
    "#         eps = torch.randn(mu.size()).cuda()\n",
    "#         action = (mu + sigma_sq.sqrt()*eps).data\n",
    "#         prob = torch.distributions.normal.Normal(mu, sigma_sq.sqrt())\n",
    "#         entropy = prob.entropy().sum(axis=-1)\n",
    "#         log_prob = prob.log_prob(action).sum(axis=-1)\n",
    "        entropy = self.dist_entropy(mu,  sigma)\n",
    "        log_prob = self.dist_log_prob(mu,  sigma, action)\n",
    "        return action, log_prob, entropy\n",
    "\n",
    "    @torch.jit.export\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma, states):\n",
    "        # Bookkeeping\n",
    "        R_EPS = 1e-9\n",
    "        R = torch.zeros(rewards.shape[0],rewards.shape[1]).cuda()\n",
    "        running_r = torch.zeros(rewards.shape[0],).cuda()\n",
    "        baseline_losses = torch.zeros(rewards.shape[1],).cuda()\n",
    "        \n",
    "        # Compute discounted cumulative sum TODO: Check this\n",
    "        for j in range(rewards.shape[1]):\n",
    "            i = rewards.shape[1] - 1 - j\n",
    "            running_r = gamma * running_r + rewards[:, i]\n",
    "            baseline_rpred = self.baseline(states[:, i])[:, 0]\n",
    "            R[:, i] = running_r - baseline_rpred # Subtract the baseline\n",
    "            baseline_loss = torch.sum((baseline_rpred - running_r)**2)\n",
    "            baseline_losses[i] = baseline_loss\n",
    "            \n",
    "        # Normalize advantages\n",
    "        R_mean = torch.mean(R)\n",
    "        R_std = torch.std(R)\n",
    "        R = (R - R_mean) / (R_std + R_EPS)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = -(log_probs*R).sum() - 0.0001*entropies.sum()\n",
    "        loss = loss / len(rewards)\n",
    "        baseline_loss = baseline_losses.sum() / len(rewards)\n",
    "        loss += baseline_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8r_Tyu_A8MFq",
    "outputId": "00e08a56-1431-4dd2-f45e-be0531dbfe1c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward: -838.8779907226562\n",
      "Episode: 1, reward: -831.6727905273438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16689/3060363218.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = 0\n",
    "gamma = torch.Tensor([0.99]).cuda()\n",
    "exploration_end = 100\n",
    "num_steps = 1000\n",
    "num_episodes = 2000\n",
    "hidden_size = 128\n",
    "num_envs = 100\n",
    "\n",
    "entry_point = functools.partial(envs.create_gym_env, env_name='reacher')\n",
    "if 'brax-reacher-v0' not in gym.envs.registry.env_specs:\n",
    "    gym.register('brax-reacher-v0', entry_point=entry_point)\n",
    "env = gym.make('brax-reacher-v0', batch_size=num_envs, episode_length=num_steps)\n",
    "env = to_torch.JaxToTorchWrapper(env, device='cuda')\n",
    "\n",
    "env.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "agent = REINFORCE(hidden_size, env.observation_space.shape[1], env.action_space)\n",
    "agent = torch.jit.script(agent)\n",
    "optimizer = optim.Adam(list(agent.model.parameters()) + list(agent.baseline.parameters()))\n",
    "    \n",
    "for i_episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    entropies = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for t in range(num_steps):\n",
    "        action, log_prob, entropy = agent.select_action(state)\n",
    "        action = action.cpu()\n",
    "    \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        entropies.append(entropy)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        state = next_state\n",
    "\n",
    "    # Bookkeeping\n",
    "    states = torch.cat([s[None] for s in states])\n",
    "    states = torch.transpose(states, 1, 0)\n",
    "    \n",
    "    actions = torch.cat([a[None] for a in actions])\n",
    "    actions = torch.transpose(actions, 1, 0)\n",
    "    \n",
    "    rewards = torch.cat([r[None] for r in rewards])\n",
    "    rewards = torch.transpose(rewards, 1, 0)\n",
    "    \n",
    "    entropies = torch.cat([e[None] for e in entropies])\n",
    "    entropies = torch.transpose(entropies, 1, 0)\n",
    "    \n",
    "    log_probs = torch.cat([lp[None] for lp in log_probs])\n",
    "    log_probs = torch.transpose(log_probs, 1, 0)\n",
    "\n",
    "    loss = agent.update_parameters(rewards, log_probs, entropies, gamma, states)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    rewards_np = rewards.cpu().numpy().sum(axis=-1).mean()\n",
    "    print(\"Episode: {}, reward: {}\".format(i_episode, rewards_np))\n",
    "    \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Brax Environments.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
