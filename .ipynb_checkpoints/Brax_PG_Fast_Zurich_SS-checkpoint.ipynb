{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishekunique/zurich-ss/blob/main/.ipynb_checkpoints/Brax_PG_Fast_Zurich_SS-checkpoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJhPpM5ZPrpq"
      },
      "outputs": [],
      "source": [
        "#@title Import Brax and some helper modules\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import math\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, List\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import metrics\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
        "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
        "v = torch.ones(1, device='cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQFCkfu8Qwre"
      },
      "source": [
        "Here is a REINFORCE agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fWJE4b5BHeH7"
      },
      "outputs": [],
      "source": [
        "class REINFORCEAgent(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               policy_layers: Sequence[int],\n",
        "               value_layers: Sequence[int],\n",
        "               discount: float, \n",
        "               entropy_weight: float,\n",
        "               device: str):\n",
        "    super(REINFORCEAgent, self).__init__()\n",
        "\n",
        "    policy = []\n",
        "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
        "      policy.append(nn.Linear(w1, w2))\n",
        "      policy.append(nn.SiLU())\n",
        "    policy.pop()  # drop the final activation\n",
        "    self.policy = nn.Sequential(*policy)\n",
        "\n",
        "    value = []\n",
        "    for w1, w2 in zip(value_layers, value_layers[1:]):\n",
        "      value.append(nn.Linear(w1, w2))\n",
        "      value.append(nn.SiLU())\n",
        "    value.pop()  # drop the final activation\n",
        "    self.value = nn.Sequential(*value)\n",
        "    self.discount = discount \n",
        "    self.entropy_weight = entropy_weight\n",
        "    self.device = device\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_create(self, logits):\n",
        "    \"\"\"Normal followed by tanh.\n",
        "\n",
        "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
        "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
        "    scale = F.softplus(scale) + .001\n",
        "    return loc, scale\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_sample_no_postprocess(self, loc, scale):\n",
        "    return torch.normal(loc, scale)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_entropy(self, loc, scale):\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    entropy = 0.5 + log_normalized\n",
        "    entropy = entropy * torch.ones_like(loc)\n",
        "    return entropy.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def dist_log_prob(self, loc, scale, dist):\n",
        "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    log_prob = log_unnormalized - log_normalized\n",
        "    return log_prob.sum(dim=-1)\n",
        "\n",
        "  @torch.jit.export\n",
        "  def get_logits_action(self, observation):\n",
        "    logits = self.policy(observation)\n",
        "    loc, scale = self.dist_create(logits)\n",
        "    action = self.dist_sample_no_postprocess(loc, scale)\n",
        "    entropy = self.dist_entropy(loc,  scale)\n",
        "    log_prob = self.dist_log_prob(loc,  scale, action)\n",
        "    return logits, action, entropy, log_prob\n",
        "\n",
        "  @torch.jit.export\n",
        "  def update_parameters(self, sample_trajs: List[torch.Tensor]):\n",
        "      states = sample_trajs[0]\n",
        "      actions = sample_trajs[1]\n",
        "      rewards = sample_trajs[2]\n",
        "      entropies = sample_trajs[3]\n",
        "      log_probs = sample_trajs[4]\n",
        "    \n",
        "      # Bookkeeping\n",
        "      R_EPS = 1e-9\n",
        "      R = torch.zeros(rewards.shape[0],rewards.shape[1]).cuda()\n",
        "      running_r = torch.zeros(rewards.shape[0],).cuda()\n",
        "      baseline_losses = torch.zeros(rewards.shape[1],).cuda()\n",
        "      \n",
        "      # Compute discounted cumulative sum TODO: Check this\n",
        "      for j in range(rewards.shape[1]):\n",
        "          i = rewards.shape[1] - 1 - j\n",
        "          running_r = self.discount * running_r + rewards[:, i]\n",
        "          baseline_rpred = self.value(states[:, i])[:, 0]\n",
        "          R[:, i] = running_r - baseline_rpred # Subtract the baseline\n",
        "          baseline_loss = torch.sum((baseline_rpred - running_r)**2)\n",
        "          baseline_losses[i] = baseline_loss\n",
        "          \n",
        "      # Normalize advantages\n",
        "      R_mean = torch.mean(R)\n",
        "      R_std = torch.std(R)\n",
        "      R = (R - R_mean) / (R_std + R_EPS)\n",
        "      \n",
        "      # Compute loss\n",
        "      loss = -(log_probs*R).sum() - self.entropy_weight*entropies.sum()\n",
        "      loss = loss / len(rewards)\n",
        "      baseline_loss = baseline_losses.sum() / len(rewards)\n",
        "      loss += baseline_loss\n",
        "      return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWbuk7IAR0SU"
      },
      "source": [
        "Finally, some code for unrolling and batching environment data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D3y5o7-oSBm-"
      },
      "outputs": [],
      "source": [
        "def sample_trajectory(agent, env, num_steps):\n",
        "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
        "  observation = env.reset()\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  entropies = []\n",
        "  log_probs = []\n",
        "  for _ in range(num_steps):\n",
        "    logits, action, entropy, log_prob = agent.get_logits_action(observation)\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    states.append(observation[None])\n",
        "    actions.append(action[None])\n",
        "    rewards.append(reward[None])\n",
        "    entropies.append(entropy[None])\n",
        "    log_probs.append(log_prob[None])\n",
        "  return [torch.transpose(torch.cat(states), 0, 1), \n",
        "          torch.transpose(torch.cat(actions), 0, 1),\n",
        "          torch.transpose(torch.cat(rewards), 0, 1), \n",
        "          torch.transpose(torch.cat(entropies), 0, 1),\n",
        "          torch.transpose(torch.cat(log_probs), 0, 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hqj0Nyo_kV-t"
      },
      "outputs": [],
      "source": [
        "def make_env(env_name, num_envs, episode_length, device):\n",
        "  gym_name = f'brax-{env_name}-v0'\n",
        "  if gym_name not in gym.envs.registry.env_specs:\n",
        "    entry_point = functools.partial(envs.create_gym_env, env_name=env_name)\n",
        "    gym.register(gym_name, entry_point=entry_point)\n",
        "  env = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QWZOn5Y3kV-u"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    env_name: str = 'reacher',\n",
        "    num_envs: int = 2048,\n",
        "    episode_length: int = 100,\n",
        "    device: str = 'cuda',\n",
        "    num_epochs: int = 200,\n",
        "    discount: float = 0.99,\n",
        "    entropy_weight: float = 0.0001, \n",
        "    hidden_size: int = 128,\n",
        "    learning_rate: float = 3e-4,\n",
        "):\n",
        "\n",
        "  # Define environment  \n",
        "  env = make_env(env_name, num_envs, episode_length, device)\n",
        "    \n",
        "  # env warmup\n",
        "  env.reset()\n",
        "  action = torch.zeros(env.action_space.shape).to(device)\n",
        "  env.step(action)\n",
        "\n",
        "  # create the agent\n",
        "  policy_layers = [env.observation_space.shape[-1], hidden_size, hidden_size, env.action_space.shape[-1] * 2]\n",
        "  value_layers = [env.observation_space.shape[-1], hidden_size, hidden_size, 1]\n",
        "  agent = REINFORCEAgent(policy_layers, value_layers, discount, entropy_weight, device)\n",
        "  agent = torch.jit.script(agent.to(device))\n",
        "  optimizer = optim.Adam(agent.parameters())\n",
        "    \n",
        "  # Bookkeeping\n",
        "  returns = []\n",
        "\n",
        "  # Training loop\n",
        "  for iter_num in range(num_epochs):\n",
        "    # Sample trajectories\n",
        "    sample_trajs = sample_trajectory(agent, env, episode_length)\n",
        "\n",
        "    loss = agent.update_parameters(sample_trajs)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    rewards_np = sample_trajs[2].cpu().numpy().sum(axis=-1).mean()\n",
        "    print(\"Episode: {}, reward: {}\".format(iter_num, rewards_np))\n",
        "    returns.append(rewards_np)\n",
        "\n",
        "  plt.plot(returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B-lrKHvkUeYM",
        "outputId": "0b951fb3-e420-4c72-e4f0-e3b76b9b07ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, reward: -118.9944839477539\n",
            "Episode: 1, reward: -115.08551788330078\n",
            "Episode: 2, reward: -110.80552673339844\n",
            "Episode: 3, reward: -107.63352966308594\n",
            "Episode: 4, reward: -103.94358825683594\n",
            "Episode: 5, reward: -100.77323150634766\n",
            "Episode: 6, reward: -96.59815216064453\n",
            "Episode: 7, reward: -93.1082763671875\n",
            "Episode: 8, reward: -89.98724365234375\n",
            "Episode: 9, reward: -86.2623291015625\n",
            "Episode: 10, reward: -82.44577026367188\n",
            "Episode: 11, reward: -79.02083587646484\n",
            "Episode: 12, reward: -75.05027770996094\n",
            "Episode: 13, reward: -72.03496551513672\n",
            "Episode: 14, reward: -68.30230712890625\n",
            "Episode: 15, reward: -64.63097381591797\n",
            "Episode: 16, reward: -60.86248779296875\n",
            "Episode: 17, reward: -57.087013244628906\n",
            "Episode: 18, reward: -53.44245147705078\n",
            "Episode: 19, reward: -49.53993225097656\n",
            "Episode: 20, reward: -46.02861785888672\n",
            "Episode: 21, reward: -42.69978332519531\n",
            "Episode: 22, reward: -39.55026626586914\n",
            "Episode: 23, reward: -36.7495231628418\n",
            "Episode: 24, reward: -34.32722473144531\n",
            "Episode: 25, reward: -32.265235900878906\n",
            "Episode: 26, reward: -30.23054313659668\n",
            "Episode: 27, reward: -28.42981719970703\n",
            "Episode: 28, reward: -27.14218521118164\n",
            "Episode: 29, reward: -25.683496475219727\n",
            "Episode: 30, reward: -24.518810272216797\n",
            "Episode: 31, reward: -23.78070831298828\n",
            "Episode: 32, reward: -22.266525268554688\n",
            "Episode: 33, reward: -21.781360626220703\n",
            "Episode: 34, reward: -20.839889526367188\n",
            "Episode: 35, reward: -19.8823184967041\n",
            "Episode: 36, reward: -19.69469451904297\n",
            "Episode: 37, reward: -18.70875358581543\n",
            "Episode: 38, reward: -19.140731811523438\n",
            "Episode: 39, reward: -18.405071258544922\n",
            "Episode: 40, reward: -22.343341827392578\n",
            "Episode: 41, reward: -18.616683959960938\n",
            "Episode: 42, reward: -18.819477081298828\n",
            "Episode: 43, reward: -16.889747619628906\n",
            "Episode: 44, reward: -18.916900634765625\n",
            "Episode: 45, reward: -19.805335998535156\n",
            "Episode: 46, reward: -20.001251220703125\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-6046556d326d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env_name, num_envs, episode_length, device, num_epochs, discount, entropy_weight, hidden_size, learning_rate)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0msample_trajs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_trajs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Brax PG Fast Zurich SS",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}