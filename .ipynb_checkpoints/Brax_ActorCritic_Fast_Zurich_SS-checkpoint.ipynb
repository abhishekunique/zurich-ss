{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhishekunique/zurich-ss/blob/main/Brax_PG_Fast_Zurich_SS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GJhPpM5ZPrpq"
   },
   "outputs": [],
   "source": [
    "#@title Import Brax and some helper modules\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from typing import Any, Callable, Dict, Optional, Sequence, List\n",
    "\n",
    "try:\n",
    "  import brax\n",
    "except ImportError:\n",
    "  !pip install git+https://github.com/google/brax.git@main\n",
    "  clear_output()\n",
    "  import brax\n",
    "\n",
    "from brax import envs\n",
    "from brax.envs import to_torch\n",
    "from brax.io import metrics\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
    "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
    "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
    "v = torch.ones(1, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Buffer to store environment transitions.\"\"\"\n",
    "    def __init__(self, obs_size, action_size, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        \n",
    "        self.obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
    "        self.next_obses = np.empty((capacity, obs_size), dtype=np.float32)\n",
    "        self.actions = np.empty((capacity, action_size), dtype=np.float32)\n",
    "        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n",
    "        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.last_save = 0\n",
    "        self.full = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.capacity if self.full else self.idx\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done):\n",
    "        np.copyto(self.obses[self.idx], obs)\n",
    "        np.copyto(self.actions[self.idx], action)\n",
    "        np.copyto(self.rewards[self.idx], reward)\n",
    "        np.copyto(self.next_obses[self.idx], next_obs)\n",
    "        np.copyto(self.not_dones[self.idx], not done)\n",
    "\n",
    "        self.idx = (self.idx + 1) % self.capacity\n",
    "        self.full = self.full or self.idx == 0\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.randint(0,\n",
    "                                 self.capacity if self.full else self.idx,\n",
    "                                 size=batch_size)\n",
    "\n",
    "        obses = torch.as_tensor(self.obses[idxs], device=self.device).float()\n",
    "        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n",
    "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n",
    "        next_obses = torch.as_tensor(self.next_obses[idxs],\n",
    "                                     device=self.device).float()\n",
    "        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n",
    "\n",
    "        return obses, actions, rewards, next_obses, not_dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQFCkfu8Qwre"
   },
   "source": [
    "Actor critic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fWJE4b5BHeH7"
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "  def __init__(self,\n",
    "               policy_layers: Sequence[int],\n",
    "               qf_layers: Sequence[int],\n",
    "               target_qf_layers: Sequence[int],\n",
    "               discount: float, \n",
    "               entropy_weight: float,\n",
    "               device: str):\n",
    "    super(Agent, self).__init__()\n",
    "\n",
    "    policy = []\n",
    "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
    "      policy.append(nn.Linear(w1, w2))\n",
    "      policy.append(nn.SiLU())\n",
    "    policy.pop()  # drop the final activation\n",
    "    self.policy = nn.Sequential(*policy)\n",
    "\n",
    "    qf = []\n",
    "    for w1, w2 in zip(qf_layers, qf_layers[1:]):\n",
    "      qf.append(nn.Linear(w1, w2))\n",
    "      qf.append(nn.SiLU())\n",
    "    qf.pop()  # drop the final activation\n",
    "    self.qf = nn.Sequential(*qf)\n",
    "    \n",
    "    target_qf = []\n",
    "    for w1, w2 in zip(target_qf_layers, target_qf_layers[1:]):\n",
    "      target_qf.append(nn.Linear(w1, w2))\n",
    "      target_qf.append(nn.SiLU())\n",
    "    target_qf.pop()  # drop the final activation\n",
    "    self.target_qf = nn.Sequential(*target_qf)\n",
    "    \n",
    "    self.discount = discount \n",
    "    self.entropy_weight = entropy_weight\n",
    "    self.device = device\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_create(self, logits):\n",
    "    \"\"\"Normal followed by tanh.\n",
    "\n",
    "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
    "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
    "    scale = F.softplus(scale) + .001\n",
    "    return loc, scale\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_sample_no_postprocess(self, loc, scale):\n",
    "    return torch.normal(loc, scale)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_entropy(self, loc, scale):\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    entropy = 0.5 + log_normalized\n",
    "    entropy = entropy * torch.ones_like(loc)\n",
    "    return entropy.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def dist_log_prob(self, loc, scale, dist):\n",
    "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
    "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
    "    log_prob = log_unnormalized - log_normalized\n",
    "    return log_prob.sum(dim=-1)\n",
    "\n",
    "  @torch.jit.export\n",
    "  def get_logits_action(self, observation):\n",
    "    logits = self.policy(observation)\n",
    "    loc, scale = self.dist_create(logits)\n",
    "    action = self.dist_sample_no_postprocess(loc, scale)\n",
    "    entropy = self.dist_entropy(loc,  scale)\n",
    "    log_prob = self.dist_log_prob(loc,  scale, action)\n",
    "    return logits, action, entropy, log_prob\n",
    "\n",
    "  # TODO: Check maximum entropy\n",
    "  @torch.jit.export\n",
    "  def compute_losses(self, obs_t, actions_t, rewards_t, next_obs_t, not_dones_t):\n",
    "    # Policy loss\n",
    "    logits = self.policy(obs_t)\n",
    "    loc, scale = self.dist_create(logits)\n",
    "    new_obs_actions = self.dist_sample_no_postprocess(loc, scale)\n",
    "\n",
    "    # TODO: Change this over to REINFORCE loss maybe instead of reparameterization??\n",
    "    q_new_actions = self.qf(torch.cat([obs_t, new_obs_actions], dim=-1))\n",
    "    policy_loss = -q_new_actions.mean()\n",
    "\n",
    "    # Compute Bellman loss\n",
    "    q1_pred = self.qf(torch.cat([obs_t, actions_t], dim=-1))\n",
    "    logits = self.policy(next_obs_t)\n",
    "    loc, scale = self.dist_create(logits)\n",
    "    new_next_actions = self.dist_sample_no_postprocess(loc, scale)\n",
    "\n",
    "    target_q_values = self.target_qf(torch.cat([next_obs_t, new_next_actions], dim=-1))\n",
    "    q_target = rewards_t + not_dones_t * self.discount * target_q_values\n",
    "\n",
    "    # L2 error on bellman\n",
    "    qf_loss = torch.linalg.norm(q1_pred - q_target.detach(), dim=-1).mean()\n",
    "\n",
    "    return policy_loss, qf_loss\n",
    "\n",
    "  @torch.jit.export\n",
    "  def update_parameters(self, sample_trajs: List[torch.Tensor]):\n",
    "      states = sample_trajs[0]\n",
    "      actions = sample_trajs[1]\n",
    "      rewards = sample_trajs[2]\n",
    "      entropies = sample_trajs[3]\n",
    "      log_probs = sample_trajs[4]\n",
    "    \n",
    "      # Bookkeeping\n",
    "      R_EPS = 1e-9\n",
    "      R = torch.zeros(rewards.shape[0],rewards.shape[1]).cuda()\n",
    "      running_r = torch.zeros(rewards.shape[0],).cuda()\n",
    "      baseline_losses = torch.zeros(rewards.shape[1],).cuda()\n",
    "      \n",
    "      # Compute discounted cumulative sum TODO: Check this\n",
    "      for j in range(rewards.shape[1]):\n",
    "          i = rewards.shape[1] - 1 - j\n",
    "          running_r = self.discount * running_r + rewards[:, i]\n",
    "          baseline_rpred = self.value(states[:, i])[:, 0]\n",
    "          R[:, i] = running_r - baseline_rpred # Subtract the baseline\n",
    "          baseline_loss = torch.sum((baseline_rpred - running_r)**2)\n",
    "          baseline_losses[i] = baseline_loss\n",
    "          \n",
    "      # Normalize advantages\n",
    "      R_mean = torch.mean(R)\n",
    "      R_std = torch.std(R)\n",
    "      R = (R - R_mean) / (R_std + R_EPS)\n",
    "      \n",
    "      # Compute loss\n",
    "      loss = -(log_probs*R).sum() - self.entropy_weight*entropies.sum()\n",
    "      loss = loss / len(rewards)\n",
    "      baseline_loss = baseline_losses.sum() / len(rewards)\n",
    "      loss += baseline_loss\n",
    "      return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_target(net, target_net, tau):\n",
    "    for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "        target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWbuk7IAR0SU"
   },
   "outputs": [],
   "source": [
    "def sample_trajectory(agent, env, num_steps, replay_buffer):\n",
    "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
    "  observation = env.reset()\n",
    "  states = []\n",
    "  actions = []\n",
    "  rewards = []\n",
    "  entropies = []\n",
    "  log_probs = []\n",
    "  for _ in range(num_steps):\n",
    "    logits, action, entropy, log_prob = agent.get_logits_action(observation)\n",
    "    next_observation, reward, done, info = env.step(action)\n",
    "    for j in range(next_observation.shape[0]):\n",
    "        replay_buffer.add(observation.cpu().detach().numpy()[j], \n",
    "                          action.cpu().detach().numpy()[j], \n",
    "                          reward.cpu().detach().numpy()[j], \n",
    "                          next_observation.cpu().detach().numpy()[j], \n",
    "                          done.cpu().detach().numpy()[j])\n",
    "    \n",
    "    states.append(observation[None])\n",
    "    actions.append(action[None])\n",
    "    rewards.append(reward[None])\n",
    "    entropies.append(entropy[None])\n",
    "    log_probs.append(log_prob[None])\n",
    "    \n",
    "    observation = next_observation\n",
    "  return [torch.transpose(torch.cat(states), 0, 1), \n",
    "          torch.transpose(torch.cat(actions), 0, 1),\n",
    "          torch.transpose(torch.cat(rewards), 0, 1), \n",
    "          torch.transpose(torch.cat(entropies), 0, 1),\n",
    "          torch.transpose(torch.cat(log_probs), 0, 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, num_envs, episode_length, device):\n",
    "  gym_name = f'brax-{env_name}-v0'\n",
    "  if gym_name not in gym.envs.registry.env_specs:\n",
    "    entry_point = functools.partial(envs.create_gym_env, env_name=env_name)\n",
    "    gym.register(gym_name, entry_point=entry_point)\n",
    "  env = gym.make(gym_name, batch_size=num_envs, episode_length=episode_length)\n",
    "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3y5o7-oSBm-"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    env_name: str = 'reacher',\n",
    "    num_envs: int = 2048,\n",
    "    episode_length: int = 100,\n",
    "    device: str = 'cuda',\n",
    "    num_epochs: int = 200,\n",
    "    discount: float = 0.99,\n",
    "    entropy_weight: float = 0.0001, \n",
    "    hidden_size: int = 128,\n",
    "    learning_rate: float = 3e-4,\n",
    "    capacity = 10000,\n",
    "    batch_size = 32,\n",
    "    target_flip_freq = 10,\n",
    "    target_flip_tau = 5e-3,\n",
    "    num_update_steps = 100\n",
    "):\n",
    "  # Define environment  \n",
    "  env = make_env(env_name, num_envs, episode_length, device)\n",
    "    \n",
    "  # env warmup\n",
    "  env.reset()\n",
    "  action = torch.zeros(env.action_space.shape).to(device)\n",
    "  env.step(action)\n",
    "\n",
    "    \n",
    "  # Define replay buffer\n",
    "  replay_buffer = ReplayBuffer(env.observation_space.shape[1], \n",
    "                               env.action_space.shape[1], \n",
    "                               capacity, \n",
    "                               device)\n",
    "\n",
    "  # create the agent\n",
    "  policy_layers = [\n",
    "      env.observation_space.shape[-1], \n",
    "      hidden_size, \n",
    "      hidden_size, \n",
    "      env.action_space.shape[-1] * 2\n",
    "  ]\n",
    "  qf_layers = [env.observation_space.shape[-1] + env.action_space.shape[-1], \n",
    "                  hidden_size, \n",
    "                  hidden_size, \n",
    "                  1]\n",
    "  target_qf_layers = [env.observation_space.shape[-1] + env.action_space.shape[-1], \n",
    "                  hidden_size, \n",
    "                  hidden_size, \n",
    "                  1]\n",
    "  agent = Agent(policy_layers, qf_layers, target_qf_layers, discount, entropy_weight, device)\n",
    "  agent = torch.jit.script(agent.to(device))\n",
    "  policy_optimizer = optim.Adam(agent.policy.parameters())\n",
    "  qf_optimizer = optim.Adam(agent.qf.parameters())\n",
    "\n",
    "  # Copy parameters initially\n",
    "  flip_target(agent.qf, agent.target_qf, 1.0)\n",
    "    \n",
    "  returns = []\n",
    "\n",
    "  for iter_num in range(num_epochs):\n",
    "    # Sample trajectories\n",
    "    sample_trajs = sample_trajectory(agent, env, episode_length, replay_buffer)\n",
    "    rewards_np = sample_trajs[2].cpu().numpy().sum(axis=-1).mean()\n",
    "    print(\"Episode: {}, reward: {}\".format(iter_num, rewards_np))\n",
    "    returns.append(rewards_np)\n",
    "    \n",
    "    # Perform actor-critic update\n",
    "    for update_num in range(num_update_steps):\n",
    "        obs_t, actions_t, rewards_t, next_obs_t, not_dones_t = replay_buffer.sample(batch_size)\n",
    "        policy_loss, qf_loss = agent.compute_losses(obs_t, actions_t, rewards_t, next_obs_t, not_dones_t)\n",
    "#         loss = policy_loss + qf_loss\n",
    "        \n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "        \n",
    "        qf_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        qf_optimizer.step()\n",
    "    \n",
    "    # Flip target network originally\n",
    "    if iter_num % target_flip_freq == 0:\n",
    "        flip_target(agent.qf, agent.target_qf, target_flip_tau)\n",
    "        \n",
    "\n",
    "  plt.plot(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "B-lrKHvkUeYM",
    "outputId": "3acec01a-935e-4ac3-fd4d-c246a4e4e85a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INVALID_ARGUMENT: DLPack tensor is on GPU, but no GPU backend was provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26311/3364925475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26311/3244142566.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env_name, num_envs, episode_length, device, num_epochs, discount, entropy_weight, hidden_size, learning_rate, capacity, batch_size, target_flip_freq, target_flip_tau, num_update_steps)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/brax/envs/to_torch.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/brax/envs/to_torch.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch_to_jax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    875\u001b[0m                             '1 positional argument')\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/brax/io/torch.py\u001b[0m in \u001b[0;36m_tensor_to_jax\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;34m\"\"\"Converts a PyTorch Tensor into a Jax DeviceArray.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_dlpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dlpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax_dlpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dlpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/anaconda3/envs/py39/lib/python3.9/site-packages/jax/_src/dlpack.py\u001b[0m in \u001b[0;36mfrom_dlpack\u001b[0;34m(dlpack)\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mgpu_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m   buf = xla_client._xla.dlpack_managed_tensor_to_buffer(\n\u001b[0m\u001b[1;32m     61\u001b[0m       dlpack, cpu_backend, gpu_backend)\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INVALID_ARGUMENT: DLPack tensor is on GPU, but no GPU backend was provided."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Brax PG Fast Zurich SS",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
